{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2841bb4e",
   "metadata": {},
   "source": [
    "# SVM iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df78ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel:  linear\n",
      "The best test size is 0.15. Score = 1.0 \n",
      "\n",
      "Kernel:  rbf\n",
      "The best test size is 0.35. Score = 0.9245 \n",
      "\n",
      "Kernel:  poly\n",
      "The best test size is 0.15. Score = 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR_CODE.  Try linear, rbf and poly kernels\n",
    "# START_CODE\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "def kernel_test(kernel, C, third):\n",
    "    the_best = (0, 0)\n",
    "    test_size = 0.15\n",
    "    while test_size < 0.9:\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=100)\n",
    "        if kernel == 'linear':\n",
    "            clf_svm = SVC(C=C, kernel=kernel, max_iter=third).fit(X_train1, y_train1)\n",
    "        if kernel == 'rbf':\n",
    "            clf_svm = SVC(C=C, kernel=kernel, gamma=third).fit(X_train1, y_train1)\n",
    "        if kernel == 'poly':\n",
    "            clf_svm = SVC(C=C, kernel=kernel, degree=third).fit(X_train1, y_train1)\n",
    "\n",
    "        score_test = clf_svm.score(X_test1, y_test1)\n",
    "        score_train = clf_svm.score(X_train1, y_train1)\n",
    "        # print(\"size = {}. Score = {}, {}\".format(round(test_size, 2), round(score_test, 4), round(score_train, 4)))\n",
    "        if score_test > the_best[0]:\n",
    "            the_best = (score_test, test_size)\n",
    "        test_size += 0.01\n",
    "    print(\"Kernel: \", kernel)\n",
    "    print(\"The best test size is {}. Score = {} \\n\".format(round(the_best[1], 2), round(the_best[0], 4)))\n",
    "\n",
    "\n",
    "kernel_test('linear', 1, 10000)\n",
    "kernel_test('rbf', 10, 0.001)\n",
    "kernel_test('poly', 5000, 3)\n",
    "# END_CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8655ac",
   "metadata": {},
   "source": [
    "# Spam classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352a3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.io import loadmat\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135a4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(fn1):\n",
    "    with open(fn1, 'r') as f:\n",
    "        content1 = f.read()\n",
    "    return content1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398378e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(content1):\n",
    "    \"\"\"\n",
    "    content: str - body of mail\n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    \"\"\"\n",
    "    # YOUR_CODE.  Split the content to tokens. You may need re.split()\n",
    "    # START_CODE\n",
    "    token = np.array(nltk.casual_tokenize(content1))\n",
    "    # END_CODE\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fcff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(token):\n",
    "    \"\"\"\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens in lower case (str)\n",
    "    \"\"\"\n",
    "    # YOUR_CODE.  Make all tokens in lower case\n",
    "    # START_CODE\n",
    "    token = np.char.lower(token)\n",
    "    # END_CODE\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176894f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(token):\n",
    "    \"\"\"\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens replaced with corresponding unified words\n",
    "\n",
    "    # YOUR_CODE.\n",
    "    # Remove html and other tags\n",
    "    # mark all numbers \"number\"\n",
    "    # mark all  urls as \"httpaddr\"\n",
    "    # mark all emails as \"emailaddr\"\n",
    "    # replace $ as \"dollar\"\n",
    "    # get rid of any punctuation\n",
    "    # Remove any non-alphanumeric characters\n",
    "    #  You may  need re.sub()\n",
    "    \"\"\"\n",
    "    # START_CODE\n",
    "    token = np.array(list(map(lambda v: re.sub(r'[>:.,?]', '', v), token)))\n",
    "    token = np.array(list(map(lambda v: re.sub(r'\\d', 'number', v), token)))\n",
    "    token = np.array(list(map(lambda v: re.sub(r'http\\S+', 'httpaddr', v), token)))\n",
    "    token = np.array(list(map(lambda v: re.sub(r'\\S+@\\S+', 'emailaddr', v), token)))\n",
    "    token = np.array(list(map(lambda v: re.sub(r'\\$', 'dollar', v), token)))\n",
    "    # END_CODE\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b96e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tokens(token):\n",
    "    \"\"\"\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of filtered tokens (str)\n",
    "    \"\"\"\n",
    "    original_tokens_len = len(token)\n",
    "    # YOUR_CODE. Keep only tokens that length >0\n",
    "    # START_CODE\n",
    "    token = token[token != '']\n",
    "    # END_CODE\n",
    "\n",
    "    print('Original len= {}\\nRemaining len= {}'.format(original_tokens_len, len(token)))\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce5ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(token):\n",
    "    \"\"\"\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    \"\"\"\n",
    "    # YOUR_CODE. replace the tokens by stemmed form. You may need PorterStemmer.stem()\n",
    "    # START_CODE\n",
    "    ps = PorterStemmer()\n",
    "    token = np.array(list(map(lambda word: ps.stem(word), token)))\n",
    "    # END_CODE\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b05ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(fn1):\n",
    "    \"\"\"\n",
    "    fn: str - full path to file\n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    \"\"\"\n",
    "    vocab_list = pd.read_table(fn1, header=None)\n",
    "    vocabl = np.array(vocab_list)[:, 1]  # first columns is index, select only words column\n",
    "    print('len(vocab)= {:,}'.format(len(vocabl)))\n",
    "    return vocabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "195d31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_features(token, vocabulary):\n",
    "    \"\"\"\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of binary values 1 if word from vocabulary is in mail 0 otherwise\n",
    "    \"\"\"\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail\n",
    "    # START_CODE\n",
    "    tokens_represented = np.zeros(len(vocabulary))\n",
    "    for i in vocabulary:\n",
    "        if i in token:\n",
    "            tokens_represented[np.where(vocabulary == i)[0]] = 1\n",
    "    # END_CODE\n",
    "\n",
    "    print('{} word(s) from vocab are in the tokens.'.format(np.sum(tokens_represented)))\n",
    "\n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c732c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(_content, _vocab):\n",
    "    \"\"\"\n",
    "    content: str - body of mail\n",
    "    vocab: ndarray of str - list of considered words\n",
    "    \"\"\"\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail\n",
    "    # START_CODE\n",
    "\n",
    "    # tokenize content\n",
    "    token = word_tokenize(_content)\n",
    "\n",
    "    # make lower case\n",
    "    token = lower_case(token)\n",
    "\n",
    "    # normalize tokens\n",
    "    token = normalize_tokens(token)\n",
    "\n",
    "    # remove zero words\n",
    "    token = filter_short_tokens(token)\n",
    "\n",
    "    # stem words\n",
    "    token = stem_tokens(token)\n",
    "\n",
    "    # convert to binary array of features\n",
    "    tokens_represented = represent_features(token, _vocab)\n",
    "    # END_CODE\n",
    "\n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d50890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spam():  # arr_binary):\n",
    "    fn1 = os.path.join(os.getcwd(), 'data/spamTrain.mat')\n",
    "\n",
    "    mat = loadmat(fn1)\n",
    "    X_train = mat['X']\n",
    "    y_train = mat['y'].ravel()\n",
    "\n",
    "    print('X_train.shape= ', X_train.shape)\n",
    "    print('y_train.shape= ', y_train.shape)\n",
    "\n",
    "    fn1 = os.path.join(os.getcwd(), 'data/spamTest.mat')\n",
    "    mat = loadmat(fn1)\n",
    "    X_test = mat['Xtest']\n",
    "    y_test = mat['ytest'].ravel()\n",
    "\n",
    "    print('X_test.shape= {}', X_test.shape)\n",
    "    print('y_test.shape= {}', y_test.shape)\n",
    "    index = 0\n",
    "    print('Sample with index  ={}: \\n{}'.format(index, X_train[index]))\n",
    "\n",
    "    clf = LinearSVC(C=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Score train= {}'.format(clf.score(X_train, y_train)))\n",
    "    print('Score test= {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a05fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab)= 1,899\n",
      "Original len= 71\n",
      "Remaining len= 62\n",
      "43.0 word(s) from vocab are in the tokens.\n",
      "X_train.shape=  (4000, 1899)\n",
      "y_train.shape=  (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvv05\\AppData\\Local\\Temp\\ipykernel_23108\\2533783076.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if i in token:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.98\n",
      "data/emailSample1.txt is Not Spam\n",
      "\n",
      "Original len= 269\n",
      "Remaining len= 242\n",
      "124.0 word(s) from vocab are in the tokens.\n",
      "X_train.shape=  (4000, 1899)\n",
      "y_train.shape=  (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvv05\\AppData\\Local\\Temp\\ipykernel_23108\\2533783076.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if i in token:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.98\n",
      "data/emailSample2.txt is Not Spam\n",
      "\n",
      "Original len= 120\n",
      "Remaining len= 115\n",
      "46.0 word(s) from vocab are in the tokens.\n",
      "X_train.shape=  (4000, 1899)\n",
      "y_train.shape=  (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvv05\\AppData\\Local\\Temp\\ipykernel_23108\\2533783076.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if i in token:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.98\n",
      "data/spamSample1.txt is Spam\n",
      "\n",
      "Original len= 43\n",
      "Remaining len= 39\n",
      "17.0 word(s) from vocab are in the tokens.\n",
      "X_train.shape=  (4000, 1899)\n",
      "y_train.shape=  (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvv05\\AppData\\Local\\Temp\\ipykernel_23108\\2533783076.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if i in token:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.98\n",
      "data/spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    tokens = '''> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors \n",
    "    you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should \n",
    "    checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe \n",
    "    yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n'''\n",
    "    \"\"\"\n",
    "    fn = os.path.join(os.getcwd(), 'data/vocab.txt')\n",
    "    vocab = get_vocabulary(fn)\n",
    "\n",
    "    for sfn in ['data/emailSample1.txt', 'data/emailSample2.txt', 'data/spamSample1.txt', 'data/spamSample2.txt']:\n",
    "        fn = os.path.join(os.getcwd(), sfn)\n",
    "        content = get_sample(fn)\n",
    "\n",
    "        # YOUR_CODE.  Preprocess the sample and get prediction 0 or 1 (1 is spam)\n",
    "        # START_CODE\n",
    "        tokens = preprocess(content, vocab)\n",
    "        prediction = check_spam().predict([tokens])\n",
    "        # END_CODE\n",
    "\n",
    "        print('{} is {}\\n'.format(sfn, ('Not Spam', 'Spam')[prediction[0]]))\n",
    "\n",
    "    print('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '=' * 50))\n",
    "    # TODO: lesson 7 - wrong matrix shape\n",
    "    #       lesson 8 - unknown shape of tokens_represented and check_spam don't return anything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195491e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
